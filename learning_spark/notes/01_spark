-----------------------------------------------------------------------------
| CHAPTER 1 - INTRO TO SPARK                                                |
-----------------------------------------------------------------------------

- The Genesis of Spark

    - Google's need for massive scale led to the creation of:

        - GFS (fault-tolerant, distributed filesystem)
        - Bigtable (scalable storage of structured data across GFS)
        - MapReduce (parallel processing over GFS and BigTable, based of functional programming)


    - MapReduce applications send computation code (map and reduce functions) to where the data resides,
        favoring data locality and cluster rack affinity rather than bringing data to code.  

      The workers in the cluster aggregate and reduce the intermediate computations and produce a final
        output from the 'reduce' function, which is written to distributed storage and is available to
        your application.  This keeps most I/O on disk, and significantly reduces network traffic.


    - The ideas in Google's GFS paper were implemented in HDFS, and the MapReduce paper's were implemented
        in Hadoop MapReduce.  Hadoop became part of the ASF in 2006.

      Hadoop had widespread adoption, but it was hard to manage and administer.  The MapReduce API was
        verbose.  Also, intermediate computations were written to disk, which was slow.


    - Since Hadoop MR lacked versatility, a lot of bespoke systems were built on top of it.  These 
        included Hive, Storm, Impala, Giraph, Drill, etc.  This made the learning curve even steeper for
        developers.


    - Researchers at UC Berkeley started working on Spark in 2009, with the goal of making processing
        simpler, faster, and easier.  It is now many orders of magnitude faster.  Apache Spark 1.0 was
        released in 2014.



- What is Apache Spark?

    - Spark is a unified engine for large-scale distributed data processing, on premises or in the cloud.


    - Spark is fast:

        1. It takes advantage of cheaper, faster commodity hardware.  It stores intermediate results in
             memory.

        2. It builds query computations as a DAG.  It's DAG scheduler and query optimizer construct an
             efficient compuational graph that can be decomposed into tasks that run in parallel across
             the cluster.

        3. It's physical execution engine, Tungsten, uses whole-stage code generation to generate compact
             code for execution.


    - Spark is easy to use:

        - Fundamental abstraction RDDs allows higher-level abstractions to be built on top.


    - Spark is modular:

        - Scala, Java, Python, R, and SQL are all supported.

        - Core components are Spark SQL, Structured Streaming, MLlib, and GraphX.


    - Spark is extensible:

        - Spark focuses on its fast, parallel computation engine rather than storage.  This is unlike 
            Apache Hadoop, which coupled storage and compute.

        - Spark can read data stored in many different sources (Hadoop, Cassandra, HBase, MongoDB, Hive,
            RDBMS's, etc.) and process it all in memory.

        - Spark's 'DataFrameReader' and 'DataFrameWriter' can also be extended to read data from other
            sources, like Kafka, Kinesis, and S3.



- Unified Analytics

    - Spark SQL

        - This module works well with structured data.  You can read data stored in an RDBMS table, or
            from file formats with structured data (ie CSV, text, JSON, Avro, ORC, Parquet), and then
            construct permanent or temporary tables in Spark.

        - When using Structured APIs (Java, Python, Scala, R), you can use SQL-like queries to query the
            data just read into a Spark DataFrame.


        - Scala example:

            // Read json file from S3 bucket into a Spark DataFrame
            spark.read.json("s3://apache_spark/data/committers.json")
              .createOrReplaceTempView("committers")

            // Issue a SQL query and return result as a Spark DF
            val results = spark.sql("""SELECT name, org, module, release, num_commits
                FROM committers WHERE module = 'mllib' AND num_commits > 10
                ORDER BY num_commits DESC""")


    - MLlib

        - MLlib is a library containing common machine learning algorithms built atop high-level DF APIs.
            The 'spark.mllib' library contained RDD-based code which is now in maintenance mode.  The
            new 'spark.ml' library is used for new applications.


        - Python example:

            # In Python
            from pyspark.ml.classification import LogisticRegression

            training = spark.read.csv("s3://...")
            test = spark.read.csv("s3://...")

            # Load training data
            lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)

            # Fit the model
            lrModel = lr.fit(training)

            # Predict
            lrModel.transform(test)


    - Structured Streaming

        - Big data developers must be able to combine and react in real time to both static and streaming
            data from engines like Kafka.  The Structured Streaming model views a stream as a continuously
            growing table, with new rows appended at the end.  Developers can query it like a static
            table.


        - Spark 3.0 extended the range of streaming sources to include Kafka, Kinesis, and HDFS-based or
            cloud storage.


        - Python example:

            # Read a stream from a local host
            from pyspark.sql.functions import explode, split

            lines = (spark
              .readStream
              .format("socket")
              .option("host", "localhost")
              .option("port", 9999)
              .load())

            # Perform transformation
            # Split the lines into words
            words = lines.select(explode(split(lines.value, " ")).alias("word"))

            # Generate running word count
            word_counts = words.groupBy("word").count()

            # Write out to the stream to Kafka
            query = (word_counts
              .writeStream
              .format("kafka")
              .option("topic", "output"))


    - GraphX

        - GraphX is a library for manipulating graphs and performing graph-parallel computations.  It
            offers standard algorithms like PageRank, Connected Components, and Triangle Counting.


        - Scala example:

            val graph = Graph(vertices, edges)
            messages = spark.textFile("hdfs://...")

            val graph2 = graph.joinVertices(messages) {
              (id, vertex, msg) => ...
            }



- Spark's Distributed Execution

    - High-Level Overview


                               /--------------------------------
                              /                                | 
             Spark Application                            /  Spark Executor
             Spark Driver       <----> Cluster Manager   
             SparkSession                                 \  Spark Executor
                             \                                 |
                              \---------------------------------


    - Spark Driver

        - The 'Spark Driver' is the part of a Spark application responsible for instantiating a 
            SparkSession.  It:

            1. Communicates with the Cluster Manager

            2. Requests resources (CPU, memory, etc.) from the Cluster Manager for Spark's Executors (JVMs) 

            3. Transforms all Spark operations into DAG computations, schedules them, and distributes 
                 their execution as tasks across the Spark executors.  Once the resources are allocated,
                 it communicates directly with the executors.


    - SparkSession

        - The 'SparkSession' is a unified conduit to all Spark operations and data.  Through this one
            construct, you can:

            - Create JVM runtime parameters
            - Define DataFrames and DataSets
            - Read from data sources
            - Access catalog metadata
            - Issue Spark SQL queries


        - In a standalone application, you can create a SparkSession using the APIs.  In the shell, the
            SparkSession is created for you automatically (access it with 'spark' or 'sc').


    - Cluster Manager

        - The 'Cluster Manager' is responsible for managing and allocating resources for the cluster of
            nodes on which your application runs.


        - Spark supports 4 cluster managers:

            - Spark standalone cluster manager
            - Hadoop YARN
            - Apache Mesos
            - Kubernetes


    - Spark Executor

        - A 'Spark Executor' runs on each worker node in the cluster.  The executors communicate with the
            driver program, and are responsible for executing tasks on the workers.


        - In most deployments, only a single executor runs per node.


    - Deployment Modes

        - Spark can run in different configurations and environments.  The cluster manager is agnostic to
            where it runs, as long as executors can fulfill resource requests.


        - Deployment modes:

            1. Local
                 - Driver and executor both run on a single host JVM
                 - Cluster manager runs on same host

            2. Standalone
                 - Driver can run on any node in cluster
                 - Each node in the cluster launches it's own executor JVM
                 - Cluster manager allocated to any node in the cluster arbitrarily

            3. YARN (Client)
                 - Driver runs on a client, not part of the cluster
                 - Executor is YARN's NodeManager container
                 - YARN's Resource Manager and ApplicationManager allocate NodeManagers for executors

            4. YARN (Cluster)
                 - Driver runs with the YARN Application Master
                 - Executor is YARN's NodeManager container
                 - YARN's Resource Manager and ApplicationManager allocate NodeManagers for executors

            5. Kubernetes
                 - Driver runs in a K8s pod
                 - Each executor runs it's own pod
                 - Kubernetes manager is the cluster manager


    - Distributed Data and Partitions

        - Actual physical data is distributed across storage as partitions residing in either HDFS or
            cloud storage.


        - While the data is distributed as partitions across the physical cluster, Spark treats each 
            partition as a high-level logical data abstraction — as a DataFrame in memory. 


        - Though this is not always possible, each Spark executor is preferably allocated a task that 
            requires it to read the partition closest to it in the network, observing data locality.


        - Partitioning allows for efficient parallelism.  Here, we break up the physical data stored
            across clusters into 8 partitions, and each executor will get one or more partitions to read
            into it's memory.

            # Break up physical data into partitions
            log_df = spark.read.text("path_to_large_text_file").repartition(8)
            print(log_df.rdd.getNumPartitions())


        - Here, we create a DataFrame of 10,000 integers distributed over 8 partitions in memory.

            # Create DataFrame distribued over partitions in memory
            df = spark.range(0, 10000, 1, 8)
            print(df.rdd.getNumPartitions())



- The Developer Experience

    - Popular Spark Use Cases

        - Processing in parallel large data sets distributed across a cluster
        - Performing ad hoc or interactive queries to explore and visualize data sets
        - Building, training, and evaluating ML models using MLlib
        - Implementing end-to-end data pipelines from myriad streams of data
        - Analyzing graph data sets and social networks