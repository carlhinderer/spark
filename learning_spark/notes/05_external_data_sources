-----------------------------------------------------------------------------
| CHAPTER 5 - SPARK SQL & DATAFRAMES - EXTERNAL DATA SOURCES                |
-----------------------------------------------------------------------------

- Spark SQL and Apache Hive

    - Shark (Spark on Hive) was one of the first interactive query engines on Hadoop systems.  It 
        demonstrated that one system could be as fast as an enterprise data warehouse while scaling as
        well as Hive/MapReduce.



- Spark SQL UDFs

    - To write a Spark SQL UDF:

        from pyspark.sql.types import LongType

        # Create cubed function
        def cubed(s):
            return s * s * s

        # Register UDF
        spark.udf.register("cubed", cubed, LongType())

        # Generate temporary view
        spark.range(1, 9).createOrReplaceTempView("udf_test")

        # Query the cubed UDF
        spark.sql("SELECT id, cubed(id) AS id_cubed FROM udf_test").show()



- Evaluation Order and Null Checking

    - Spark SQL (which includes SQL, the DataFrame API, and the Dataset API) does not guarantee the order
        of evaluation of subexpressions.  


    - For example, this query does not guarantee that the null check happens before the strlen call:

        spark.sql("SELECT s FROM test1 WHERE s IS NOT NULL AND strlen(s) > 1")


    - To perform proper null checking, you should either:

        1. Make the UDF itself null-aware and do null checking inside the UDF

        2. Use IF or CASE WHEN expressions to do the null check and invoke the UDF in a conditional branch.



- Speeding Up and Distributing PySpark UDFs with Pandas UDFs

    - One previous issue with PySpark UDFs is that they had worse performance than Scala UDFs.  This was
        because Spark UDFs required data movement between the JVM and Python, which was quite expensive.


    - To resolve this problem, Pandas UDFs (aka vectorized UDFs) were introduce in Spark 2.3.  They use
        Apache Arrow to transfer data and Pandas to work with the data.  You define a pandas UDF by using
        the 'pandas_udf' decorator:

        import pandas as pd

        from pyspark.sql.functions import col, pandas_udf
        from pyspark.sql.types import LongType

        # Declare the cubed function
        def cubed(a: pd.Series) -> pd.Series:
            return a * a * a

        # Create the pandas UDF for the cubed function
        cubed_udf = pandas_udf(cubed, returnType=LongType())


    - Now, we can call the Pandas function.  The Panda Function API lets us call methods where both the
        input and output are pandas DFs.

        # Create a pandas Series
        x = pd.Series([1, 2, 3])

        # Use our UDF with local pandas data
        print(cubed(x))

        # Now create a Spark DF
        df = spark.range(1, 4)

        # Use our UDF on a Spark DF
        print(cubed(x))



- Querying with the Spark SQL Shell

    - The spark-sql CLI is a convenient tool for executing Spark SQL queries.  While this utility 
        communicates with the Hive metastore service in local mode, it does not talk to the
        Thrift JDBC/ODBC server (aka the 'Spark Thrift Server').


    - The STS allows JDBC/ODBC clients to execute SQL queries over JDBC and ODBC protocols on Spark.


    - To start the Spark SQL client:

        $ ./bin/spark-sql


    - To create a new permanent Spark SQL table:

        spark-sql> CREATE TABLE people (name STRING, age int);

      The new table will be created in the 'spark-warehouse' directory.


    - To insert data into the table:

        spark-sql> INSERT INTO people VALUES ("Michael", NULL);
        spark-sql> INSERT INTO people VALUES ("Andy", 30);
        spark-sql> INSERT INTO people VALUES ("Samantha", 19);


    - Running a Spark SQL query:

        # Show the tables in our metastore
        spark-sql> SHOW TABLES;

        # Run some queries
        spark-sql> SELECT * FROM people WHERE age < 20;
        spark-sql> SELECT name FROM people WHERE age IS NULL;



- Working with Beeline

    - Beeline is a common utility for running HiveQL queries against HiveServer2.  It is a JDBC client.
        We can use it to execute Spark SQL queries against the Spark Thrift Server.


    - To start the Thrift server:

        # Start the Spark driver and worker first if not already done
        $ ./sbin/start-all.sh

        # Start the Thrift server
        $ ./sbin/start-thriftserver.sh


    - Next, start Beeline:

        # Start Beeline
        $ ./bin/beeline

        # Configure Beeline to conect to the local Thrift server
        !connect jdbc:hive2://localhost:10000


    - To execute a Spark SQL query from Beeline:

        jdbc:hive2://localhost:10000> SHOW tables;

        jdbc:hive2://localhost:10000> SELECT * FROM people;


    - To stop the Thrift server:

        $ ./sbin/stop-thriftserver.sh



- Working with Tableau

    - Similarly to running queries through Beeline or the Spark SQL CLI, you can connect your favorite
        BI tool to Spark SQL via the Thrift JDBC/ODBC server.


    - These are the steps to connect:

        1. Start the Spark Thrift Server

        2. Go to 'Connect > Spark SQL' in Tableau Desktop and add the connection details

        3. Select a schema and a table to query

        4. Now, we can execute queries against the Spark data source, join tables, etc.


    - Remember to stop the Thrift server when you are done.



- JDBC and SQL Databases

    - Spark SQL has a data source API that can read data from other databases using JDBC.  To get started,
        you'll need to specify the JDBC driver for the source you want to read using the Spark classpath:

        $ /.bin/spark-shell --driver-class-path $database.jar --jars $database.jar


    - When transferring large amounts of data between Spark SQL and JDBC, it's important to partition
        your data source, instead of all the data going through one driver connection.  To do this, we
        set:

        numPartitions             # Max number of concurrent JDBC connections
        partitionColumn           # Column used to determine the partitions
        lowerBound                # Minimum value of partitionColumn
        upperBound                # Maximum value of partitionColumn


    - For example, if we have the following settings:

        numPartitions = 10
        lowerBound = 1000
        upperBound = 10000

      Then we will get 10 queries:

        SELECT * FROM table WHERE partitionColumn BETWEEN 1000 AND 2000
        ...
        SELECT * FROM table WHERE partitionColumn BETWEEN 9000 AND 10000


    - Tips for partitioning:

        - A good starting point is to use a multiple of the number of worker nodes.  If you have 4 worker
            nodes, use 8 partitions.

        - Use actual lower and upper bounds.  If your range is larger than the actual range, most of the
            work will be in a few partitions.

        - Choose a uniformly distributed partition column or create a new one to evenly distributed your
            partitions.



- PostgreSQL

    - To connect to a PostgreSQL database, build or download the JDBC jar and add it to your classpath.  To
        start a Spark shell specifying the jar:

        $ bin/spark-shell --jars postgresql-42.2.6.jar


    - To read data from a JDBC source using the 'load()' method:

        jdbcDF1 = (spark
            .read
            .format("jdbc")
            .option("url", "jdbc:postgresql://[DBSERVER]")
            .option("dbtable", "[SCHEMA].[TABLENAME]")
            .option("user", "[USERNAME]")
            .option("password", "[PASSWORD]")
            .load())


    - To read data from a JDBC source using the 'jdbc' method:

        jdbcDF2 = (spark
            .read
            .jdbc("jdbc:postgresql://[DBSERVER]", "[SCHEMA].[TABLENAME]",
                  properties={"user": "[USERNAME]", "password": "[PASSWORD]"}))


    - To write data to a JDBC source using the 'save()' method:

        (jdbcDF1
            .write
            .format("jdbc")
            .option("url", "jdbc:postgresql://[DBSERVER]")
            .option("dbtable", "[SCHEMA].[TABLENAME]")
            .option("user", "[USERNAME]")
            .option("password", "[PASSWORD]")
            .save())


    - To write data using the 'jdbc' method:

        (jdbcDF2
            .write
            .jdbc("jdbc:postgresql:[DBSERVER]", "[SCHEMA].[TABLENAME]",
                  properties={"user": "[USERNAME]", "password": "[PASSWORD]"}))



- MySQL

    - To connect to a MySQL database, build or download the JDBC jar and add it to your classpath.  To
        start a Spark shell:

        $ bin/pyspark --jars mysql-connector-java_8.0.16-bin.jar


    - To read data from a MySQL database:

        jdbcDF = (spark
            .read
            .format("jdbc")
            .option("url", "jdbc:mysql://[DBSERVER]:3306/[DATABASE]")
            .option("driver", "com.mysql.jdbc.Driver")
            .option("dbtable", "[TABLENAME]")
            .option("user", "[USERNAME]")
            .option("password", "[PASSWORD]")
            .load())


    - To write data to a MySQL database:

        (jdbcDF
            .write
            .format("jdbc")
            .option("url", "jdbc:mysql://[DBSERVER]:3306/[DATABASE]")
            .option("driver", "com.mysql.jdbc.Driver")
            .option("dbtable", "[TABLENAME]")
            .option("user", "[USERNAME]")
            .option("password", "[PASSWORD]")
            .save())



- Other External Data Sources

    - Azure Cosmos DB
    - Ms SQL Server
    - Apache Cassandra
    - Snowflake
    - MongoDB
    - Many others...



- Higher-Order Functions in DFs and Spark SQL

    - There are 2 typical solutions for manipulating complex data types:

        1. Explode the nested structure into individual rows, apply some function, then recreate the 
             nested structure.

        2. Build a UDF


    - For option 1, to explode and collect, we 'explode(values)' which creates a new row (with the id)
        for each element in 'values':

        SELECT id, collect_list(value + 1) AS values
        FROM (SELECT id, EXPLODE(values) AS value
              FROM table) x
        GROUP BY id

      Here, the GROUP BY statement requires shuffle operations, so this approach could be very expensive.


    - For option 2, to perform the same task (add 1 to each element), we can create a UDF that uses 'map()'
        to iterate through each element and perform the addition.  This doesn't require a shuffle, but
        may run out of memory on large data sets.

        spark.sql("SELECT id, plusOneInt(values) AS values FROM table").show()



- Built-in Functions for Complex Data Types

    - Instead of using these potentially expensive techniques, you may be able to use some of the built-in
        functions for complex data types.


    - Array functions:

        array_distinct(array<T>): array<T>
        array_intersect(array<T>, array<T>): array<T>
        array_union(array<T>, array<T>): array<T>
        array_except(array<T>, array<T>): array<T>
        array_join(array<String>, String[, String]): String

        array_max(array<T>): T
        array_min(array<T>): T

        array_position(array<T>, T): Long
        array_remove(array<T>, T): array<T>

        array_position(array<T>, T): Long
        array_remove(array<T>, T): array<T>
        arrays_overlap(array<T>, array<T>): array<T>
        array_sort(array<T>): array<T>
        concat(array<T>, ...): array<T>
        flatten(array<array<T>>): array<T>
        array_repeat(T, Int): array<T>
        reverse(array<T>): array<T>
        sequence(T, T[, T]): array<T>
        shuffle(array<T>): array<T>
        slice(array<T>, Int, Int): array<T>
        array_zip(array<T>, array<U>, ...): array<struct<T, U, ...>>

        element_at(array<T>, Int): T
        cardinality(array<T>): Int


    - Map functions:

        map_form_arrays(array<K>, array<V>): map<K, V>
        map_from_entries(array<struct<K, V>>): map<K, V>
        map_concat(map<K, V>, ...): map<K, V>
        element_at(map<K, V>, K): V
        cardinality(array<T>): Int



- Higher-Order Functions

    - In addition to the built-in functions, there are higher-order functions that take lambda functions
        as arguments.

        # In SQL
        transform(values, value -> lambda expression)


    - First, we'll create a sample data set to run some examples:

        from pyspark.sql.types import *
        schema = StructType([StructField("celsius", ArrayType(IntegerType()))])

        t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]
        t_c = spark.createDataFrame(t_list, schema)
        t_c.createOrReplaceTempView("tC")
        
        # Show the DataFrame
        t_c.show()
        

    - The 'transform()' function produces an array by applying a function to each element in the input
        array.

        spark.sql("""
            SELECT celsius,
                   transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit
            FROM tC
        """).show()


    - The 'filter()' function produces an array consisting of only the elements that return True for a
        condition.

        spark.sql("""
            SELECT celsius,
                   filter(celsius, t -> t > 38) as high
            FROM tC
        """).show()


    - The 'exists()' function returns true if the Boolean function holds for any element in the input array.

        spark.sql("""
            SELECT celsius,
                   exists(celsius, t -> t = 38) as threshold
            FROM tC
        """).show()


    - The 'reduce()' function reduces the elements of the array to a single value by merging the elements
        into a single value.

        spark.sql("""
        SELECT celsius,
               reduce(
                   celsius,
                   0,
                  (t, acc) -> t + acc,
                  acc -> (acc div size(celsius) * 9 div 5) + 32
                ) as avgFahrenheit
        FROM tC
        """).show()



- Common DataFrames and Spark SQL Operations

    - There is a extensive list of DF operations available.  Consult the Spark documentation if you're
        looking for something:

        - Aggregate functions
        - Collection functions
        - Datetime functions
        - Math functions
        - Miscellaneous functions
        - Non-aggregate functions
        - Sorting functions
        - String functions
        - UDF functions
        - Window functions


    - First, we'll set up some data for examples:

        1. Import 2 files and create 2 DFs, one for airports and one for US flight delays.

        2. Convert the delay and distance columns from STRING to INT.

        3. Create a smaller table that only focuses on flights from SEA for SFO for a small time range.


    - To set this up:

        from pyspark.sql.functions import expr
        
        tripdelaysFilePath = "/databricks-datasets/learning-spark-v2/flights/departuredelays.csv"
        airportsnaFilePath = "/databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt"

        # Obtain airports data set
        airportsna = (spark.read
            .format("csv")
            .options(header="true", inferSchema="true", sep="\t")
            .load(airportsnaFilePath))

        airportsna.createOrReplaceTempView("airports_na")

        # Obtain departure delays data set
        departureDelays = (spark.read
            .format("csv")
            .options(header="true")
            .load(tripdelaysFilePath))

        departureDelays = (departureDelays
            .withColumn("delay", expr("CAST(delay as INT) as delay"))
            .withColumn("distance", expr("CAST(distance as INT) as distance")))

        departureDelays.createOrReplaceTempView("departureDelays")

        # Create temporary small table
        foo = (departureDelays
            .filter(expr("""origin == 'SEA' and destination == 'SFO' and
                         date like '01010%' and delay > 0""")))

        foo.createOrReplaceTempView("foo")


    - To look at the data:

        # Airports data set
        spark.sql("SELECT * FROM airports_na LIMIT 10").show()

        # Departure delays data set
        spark.sql("SELECT * FROM departureDelays LIMIT 10").show()

        # Smaller data set
        spark.sql("SELECT * FROM foo").show()



- Unions

    - To union 2 different datasets with the same schema together, we use the 'union()' function.

        # Union two tables
        bar = departureDelays.union(foo)
        bar.createOrReplaceTempView("bar")

        # Show the union (filtering for SEA and SFO in specified time range)
        bar.filter(expr("""origin == 'SEA' AND destination == 'SFO'
                           AND date LIKE '01010%' AND delay > 0""")).show()


    - As expected, when we use the same filtering criteria, we see a duplication of the 'foo' data:

        spark.sql("""
        SELECT *
          FROM bar
        WHERE origin = 'SEA'
          AND destination = 'SFO'
          AND date LIKE '01010%'
          AND delay > 0
        """).show()



- Joins

    - By default, a Spark join is an inner join.  The complete list of options is:

        [inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, left_anti]


    - To join the airports and 'foo' DFs:

        # Join departure delays data (foo) with airport info
        foo.join(
            airports,
            airports.IATA == foo.origin
        ).select("City", "State", "date", "delay", "distance", "destination").show()

        # In SQL
        spark.sql("""
        SELECT a.City, a.State, f.date, f.delay, f.distance, f.destination
        FROM foo f
        JOIN airports_na a
        ON a.IATA = f.origin
        """).show()



- Windowing

    - A 'window function' uses values from the rows in a window (a range of input rows) to return a set
        of values in the form of another row.  It is a way to operate on a group of rows, while still
        returning a value for each input row.


    - Window Functions

                                SQL               DF API
        --------------------------------------------------------                        
        Ranking functions       rank()            rank()
                                dense_rank()      denseRank()
                                percent_rank()    percentRank()
                                ntile()           ntile()
                                row_number()      rowNumber()

        Analytic functions      cume_dist()       cumeDist()
                                first_value()     firstValue()
                                last_value()      lastValue()
                                lag()             lag()
                                lead()            lead()


    - Let's look at 'TotalDelays' (calculated by sum(Delay)) experienced by flights originating from SEA,
        SFO, and JFK going to specific destinations:

        # In SQL
        DROP TABLE IF EXISTS departureDelaysWindow;

        CREATE TABLE departureDelaysWindow AS
        SELECT origin, destination, SUM(delay) AS TotalDelays
        FROM departureDelays
        WHERE origin IN ('SEA', 'SFO', 'JFK')
        AND destination IN ('SEA', 'SFO', 'JFK', 'DEN', 'ORD', 'LAX', 'ATL')
        GROUP BY origin, destination;

        SELECT * FROM departureDelaysWindow;


    - Now, you want to find the 3 destinations that experienced the most delays.  You can accomplish this
        by running 3 different queries (one for each origin), then unioning the results together.

        # For one of the origins
        SELECT origin, destination, SUM(TotalDelays) AS TotalDelays
        FROM departureDelaysWindow
        WHERE origin = '[ORIGIN]'
        GROUP BY origin, destination
        ORDER BY SUM(TotalDelays) DESC
        LIMIT 3


    - But a better approach would be to use a Window function like 'dense_rank()' to perform the 
        calculation:

        # In SQL
        spark.sql("""
          SELECT origin, destination, TotalDelays, rank
          FROM (
            SELECT origin, destination, TotalDelays, dense_rank()
            OVER (PARTITION BY origin ORDER BY TotalDelays DESC) as rank
                 FROM departureDelaysWindow
          ) t
          WHERE rank <= 3
        """).show()



- Modifications

    - Another common operation is to perform modifications to the DF.  We'll start with our previous 'foo'
        DF.


    - To add a new column to the DF:

        from pyspark.sql.functions import expr

        foo2 = (foo.withColumn(
                  "status", expr("CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END")
        ))

        foo2.show()


    - To drop a column from the DF:

        foo3 = foo2.drop("delay")
        foo3.show()


    - To rename a column from the DF:

        foo4 = foo3.withColumnRenamed("status", "flight_status")
        foo4.show()


    - Sometimes, you need to 'pivot' your data, meaning to swap columns for rows.

        # Initial data
        SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay
        FROM departureDelays
        WHERE origin = 'SEA'

        # Results
        +-----------+-----+-----+
        |destination|month|delay|
        +-----------+-----+-----+
        | ORD       | 1   | 92  |


        # Now, place names in month column and perform aggregates by destination and month
        SELECT * FROM (
          SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay
          FROM departureDelays WHERE origin = 'SEA'
        )
        PIVOT (
          CAST(AVG(delay) AS DECIMAL(4, 2)) AS AvgDelay, MAX(delay) AS MaxDelay
          FOR month IN (1 JAN, 2 FEB)
        )
        ORDER BY destination

        # Results
        +-----------+------------+------------+------------+------------+
        |destination|JAN_AvgDelay|JAN_MaxDelay|FEB_AvgDelay|FEB_MaxDelay|
        +-----------+------------+------------+------------+------------+
        | ABQ       | 19.86      | 316        | 11.42      | 69         |