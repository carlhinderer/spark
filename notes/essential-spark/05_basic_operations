-------------------------------------------------------------------------------
CHAPTER 5 - BASIC STRUCTURED OPERATIONS
-------------------------------------------------------------------------------

- DataFrames and Schemas

    # Create a DataFrame to work with
    > df = spark.
               read.
               format("json").
               load("/data/flight-data/json/2015-summary.json")
    
    # Look at the schema of the dataframe
    > df.printSchema()
    
    
    
    # Check the schema during loading
    # The schema has a name, type, nullable, and optional metadata for each column
    > spark.
          read.
          format("json").
          load("/data/flight-data/json/2015-summary.json").
          schema
    
    StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),
                    StructField(ORIGIN_COUNTRY_NAME,StringType,true),
                    StructField(count,LongType,true)))
    
    
    
    # Manually specify the schema
    > from pyspark.sql.types import StructField, StructType, StringType, LongType
    
    > myManualSchema = StructType([
                           StructField("DEST_COUNTRY_NAME", StringType(), True),
                           StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
                           StructField("count", LongType(), False, metadata={"hello":"world"})
                       ])
    
    > df = spark.
               read.
               format("json").
               schema(myManualSchema).
               load("/data/flight-data/json/2015-summary.json")



- Columns and Rows

    # Refer to columns
    > from pyspark.sql.functions import col, column
    > col("someColumnName")
    > column("someColumnName")
    
    # Explicitly refer to column on a single dataframe (used for joins)
    > df.col("count")
    
    
    
    # Columns are expressions
    > from pyspark.sql.functions import expr
    
    > expr("(((someCol + 5) * 200) - 6) < otherCol")
    
    
    
    # Get all columns in a DataFrame
    > spark.
          read.
          format("json").
          load("/data/flight-data/json/2015-summary.json").
          columns
    
    
    
    # Get the first row in a DataFrame
    > df.first()
    
    
    
    # Create a row
    > from pyspark.sql import Row
    > myRow = Row("Hello", None, 1, False)
    
    
    
    # Access values of rows
    > myRow[0]
    > myRow[2]



- DataFrame Transformations

    - DataFrame Transformations:
          
        1. We can add rows or columns
        2. We can remove rows or columns
        3. We can transform a row into a column (or vice versa)
        4. We can change the order of rows based on the values in columns
    
    
    
    # Create a DataFrame from a raw source and create a temporary view to query
    > df = spark.read.format("json").load("/data/flight-data/json/2015-summary.json")
    
    > df.createOrReplaceTempView("dfTable")
    
    
    
    # Create rows manually
    > from pyspark.sql import Row
    > from pyspark.sql.types import StructField, StructType, StringType, LongType
    
    > myManualSchema = StructType([
                                   StructField("some", StringType(), True),
                                   StructField("col", StringType(), True),
                                   StructField("names", LongType(), False)
                                  ])
    
    > myRow = Row("Hello", None, 1)
    > myDf = spark.createDataFrame([myRow], myManualSchema)
    > myDf.show()




- Selecting From DataFrames

    # Get 2 destination country names
    > df.select("DEST_COUNTRY_NAME").show(2)
    
    # In SQL instead
    > spark.sql('SELECT DEST_COUNTRY_NAME FROM dfTable LIMIT 2').show()
    
    +-----------------+
    |DEST_COUNTRY_NAME|
    +-----------------+
    |    United States|
    |    United States|
    +-----------------+
    
    
    
    # Select multiple columns
    > df.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME").show(2)
    
    # In SQL
    > SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME FROM dfTable LIMIT 2
    
    +-----------------+-------------------+
    |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|
    +-----------------+-------------------+
    |    United States|            Romania|
    |    United States|            Croatia|
    +-----------------+-------------------+



    # There are several interchangable ways to refer to columns
    > from pyspark.sql.functions import expr, col, column
    
    > df.select(
             expr("DEST_COUNTRY_NAME"),
             col("DEST_COUNTRY_NAME"),
             column("DEST_COUNTRY_NAME")).show(2)
    
    
    
    # Note that you cannot use columns and strings in the same expression, 
    #   this is a common mistake that will generate a compiler error
    > df.select(col("DEST_COUNTRY_NAME"), "DEST_COUNTRY_NAME")
    
    
    
    # Column names can be aliased in an expression
    > df.select(expr("DEST_COUNTRY_NAME AS destination")).show(2)
    
    # In SQL
    > SELECT DEST_COUNTRY_NAME as destination FROM dfTable LIMIT 2
    
    
    
    # Your expression can be further manipulated as another expression
    > df.select(expr("DEST_COUNTRY_NAME as destination").
         alias("DEST_COUNTRY_NAME")).
         show(2)
    
    
    
    # Because 'select', followed by a series of 'expr', is so common, Spark has the
    #   'selectExpr' shorthand for it
    > df.selectExpr("DEST_COUNTRY_NAME as newColumnName", "DEST_COUNTRY_NAME").show(2)
    
    
    
    # Adds a new column that specifies whether the origin and destination columns are
    #   the same
    > df.selectExpr(
                    "*", # all original columns
                    "(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry").
                    show(2)
    
    # In SQL
    > SELECT *, (DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry
      FROM dfTable
      LIMIT 2
    
    +-----------------+-------------------+-----+-------------+
    |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|
    +-----------------+-------------------+-----+-------------+
    |    United States|            Romania|   15|        false|
    |    United States|            Croatia|    1|        false|
    +-----------------+-------------------+-----+-------------+



- Advanced DataFrame Selection

    # Specify an aggregation over the entire DataFrame
    > df.selectExpr("avg(count)", "count(distinct(DEST_COUNTRY_NAME))").show(2)
    
    # In SQL
    > df.selectExpr("avg(count)", "count(distinct(DEST_COUNTRY_NAME))").show(2)
    
    +-----------+---------------------------------+
    | avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|
    +-----------+---------------------------------+
    |1770.765625|                              132|
    +-----------+---------------------------------+
    
    
    
    # Convert a Python literal into a Spark type
    > from pyspark.sql.functions import lit
    > df.select(expr("*"), lit(1).alias("One")).show(2)
    
    # In SQL
    > SELECT *, 1 as One FROM dfTable LIMIT 2
    
    +-----------------+-------------------+-----+---+
    |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|
    +-----------------+-------------------+-----+---+
    |    United States|            Romania|   15|  1|
    |    United States|            Croatia|    1|  1|
    +-----------------+-------------------+-----+---+



- Adding, Changing, and Dropping DataFrame Columns

    # Add a column with 'withColumn' instead
    > df.withColumn("numberOne", lit(1)).show(2)
    
    # In SQL
    > SELECT *, 1 as numberOne FROM dfTable LIMIT 2
    
    +-----------------+-------------------+-----+---------+
    |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|
    +-----------------+-------------------+-----+---------+
    |    United States|            Romania|   15|        1|
    |    United States|            Croatia|    1|        1|
    +-----------------+-------------------+-----+---------+
    
    
    
    # Add a column with a more interesting expression
    > df.withColumn("withinCountry", expr("ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME")).
         show(2)
    
    
    
    # We can also use the 'withColumnRenamed' method to rename a column
    > df.withColumnRenamed("DEST_COUNTRY_NAME", "dest")
    
    
    
    # Drop columns from a DataFrame
    > df.drop("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME")
    
    
    
    # Change a column's type
    > df.withColumn("count2", col("count").cast("long"))
    
    # In SQL
    > SELECT *, cast(count as long) AS count2 FROM dfTable



- Filtering DataFrames

    # Both the 'filter' and 'where' methods can be used to filter rows
    > df.filter(col("count") < 2).show(2)
    > df.where("count < 2").show(2)
    
    # In SQL
    > SELECT * FROM dfTable WHERE count < 2 LIMIT 2
    
    +-----------------+-------------------+-----+
    |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
    +-----------------+-------------------+-----+
    |    United States|            Croatia|    1|
    |    United States|          Singapore|    1|
    +-----------------+-------------------+-----+
    
    
    
    # Chain filters together
    > df.where(col("count") < 2).
         where(col("ORIGIN_COUNTRY_NAME") != "Croatia").
         show(2)
    
    # In SQL
    > SELECT * 
      FROM dfTable 
      WHERE count < 2 
      AND ORIGIN_COUNTRY_NAME != "Croatia"
      LIMIT 2
    
    +-----------------+-------------------+-----+
    |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
    +-----------------+-------------------+-----+
    |    United States|          Singapore|    1|
    |          Moldova|      United States|    1|
    +-----------------+-------------------+-----+



    # Get distinct values from a column
    > df.select("ORIGIN_COUNTRY_NAME").distinct().count()
    
    # In SQL
    > SELECT COUNT(DISTINCT ORIGIN_COUNTRY_NAME) FROM dfTable
    
    # Get distinct values from multiple columns
    > df.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME").distinct().count()
    
    # In SQL
    > SELECT COUNT(DISTINCT(ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME)) FROM dfTable
    
    
    
    # Get a random sample of rows
    > seed = 5
    > withReplacement = False
    > fraction = 0.5
    > df.sample(withReplacement, fraction, seed).count()



- Splitting and Combining DataFrames

    # Split a DataFrame into sets
    > dataFrames = df.randomSplit([0.25, 0.75], seed)
    > dataFrames[0].count() > dataFrames[1].count() # False
    
    
    
    # Concatenate 2 DataFrames together, they must have the same schema
    > from pyspark.sql import Row
    > schema = df.schema
    > newRows = [
          Row("New Country", "Other Country", 5L),
          Row("New Country 2", "Other Country 3", 1L)
                ]
    
    > parallelizedRows = spark.sparkContext.parallelize(newRows)
    > newDF = spark.createDataFrame(parallelizedRows, schema)
    
    > df.union(newDF).
         where("count = 1").
         where(col("ORIGIN_COUNTRY_NAME") != "United States").
         show()



- Sorting DataFrames

    # The 'sort' and 'orderBy' methods can both be used to sort rows by a column value
    > df.sort("count").show(5)
    > df.orderBy("count", "DEST_COUNTRY_NAME").show(5)
    > df.orderBy(col("count"), col("DEST_COUNTRY_NAME")).show(5)
    
    
    
    # Specify ascending or descending sort
    # Note that asc_nulls_first, desc_nulls_first, asc_nulls_last, or desc_nulls_last
    #   can also be used to specify where nulls should go in the sort
    > from pyspark.sql.functions import desc, asc
    
    > df.orderBy(expr("count desc")).show(2)
    > df.orderBy(col("count").desc(), col("DEST_COUNTRY_NAME").asc()).show(2)
    
    
    
    # Sort each partition independently for optimization purposes
    > spark.read.
            format("json").
            load("/data/flight-data/json/*-summary.json").
            sortWithinPartitions("count")



- Limiting Results

    # Limit the number of rows that will be returned
    > df.limit(5).show()
    
    # In SQL
    > SELECT * FROM dfTable LIMIT 6



- Repartitioning and Coalescing

    # Repartition a DataFrame (will do a full shuffle)
    > df.repartition(5)
    
    # Coalesce (will try to avoid a full shuffle)
    > df.repartition(5, col("DEST_COUNTRY_NAME")).coalesce(2)
