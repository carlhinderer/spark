-----------------------------------------------------------------------
|  CHAPTER 2 - ARCHITECTURE & FLOW                                    |
-----------------------------------------------------------------------

- When you run a Spark application, your application (the 'driver') connects to 
    Spark (the 'master').  In this chapter, we'll read a CSV file, transform the
    data, and store the results in a Postgres database.



- Prepare Postgres Database

    # Log into Postgres console
    $ sudo su - postgres
    $ psql

    # Create new database
    > CREATE DATABASE sparkdb;

    # Create new user accout to access database
    > CREATE USER sparkuser WITH PASSWORD 'sparkpw';

    # Grant rights on database to user
    > GRANT ALL PRIVILEGES ON DATABASE sparkdb TO sparkuser;



- Getting Postgres Driver to Work

    1. Had to download the Postgres JDBC driver and put it at 
        /usr/share/java/postgresql-42.2.18.jar

    2. Had to pass the location of the jar to spark-submit
        $ spark-submit --jars /usr/share/java/postgresql-42.2.18.jar CsvToRelational.py

    3. For some reason, I can't get the insert into the database to complete if I create
         the table in psql.  It only works if the table is created by the JDBC write.
         Once it exists, it can be appended to.



- Connecting to a Master

    - For every Spark application, the first step is to connect to the Spark master and get
        a Spark session.  In this context, we can connecting to Spark in local mode.



- Ingesting

    - When you tell Spark to load the CSV file, the Master does not do the work.  Spark
        has the 'workers' (or 'slaves') do the work.  

    - In a cluster, a distributed ingestion will occur.  Each worker will read a portion of
        the CSV file and store it in its memory.



-Transforming

    - The transformations on the CSV rows will occur on the rows in memory where they are 
        stored on each of the workers.



- Saving to the Database

    - Each worker creates its owner connection to the database and saves its results.  We have
        to be mindful that if we have too many workers, there may be too many simultaneous
        connections to the database.



- In Summary

    - The whole dataset never hits our application (the driver).
    
    - The entire processing takes place in the workers.

    - The workers save the data in their partitions to the database.