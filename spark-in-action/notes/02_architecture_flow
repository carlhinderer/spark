-----------------------------------------------------------------------
|  CHAPTER 2 - ARCHITECTURE & FLOW                                    |
-----------------------------------------------------------------------

- When you run a Spark application, your application (the 'driver') connects to 
    Spark (the 'master').  In this chapter, we'll read a CSV file, transform the
    data, and store the results in a Postgres database.



- Prepare Postgres Database

    # Log into Postgres console
    $ sudo su - postgres
    $ psql

    # Create new database
    > CREATE DATABASE sparkdb;

    # Create new user accout to access database
    > CREATE USER sparkuser WITH PASSWORD 'sparkpw';

    # Grant rights on database to user
    > GRANT ALL PRIVILEGES ON DATABASE sparkdb TO sparkuser;



- Adding the ch02 Table

    1. The 'ch02' table needs to be created with 'sparkuser' as the owner.  By default,
         you cannot switch users in Postgres since UNIX accounts are used for 
         authentication, and you are logged in as the 'postgres' user.  To change this so you 
         can log in as users and type in the passwords, need to change the 
         '/etc/postgresql/10/main/pg_hba.conf' file.

         # Old entry in pg_hba.conf
         --------------------------------
         # TYPE DATABASE USER ADDRESS METHOD
         local  all      all          peer


         # New entry in pg_hba.conf
         --------------------------------
         # TYPE DATABASE USER ADDRESS METHOD
         local  all      all          md5


    2. Now, we can create the table we need.

        > \c sparkdb sparkuser
        > CREATE TABLE ch02(lname text, fname text, name text);



- Getting Postgres Driver to Work

    1. Had to download the Postgres JDBC driver and put it at 
        /usr/share/java/postgresql-42.2.18.jar

    2. Had to pass the location of the jar to spark-submit
        $ spark-submit --jars /usr/share/java/postgresql-42.2.18.jar csv_to_relational.py



- Connecting to a Master

    - For every Spark application, the first step is to connect to the Spark master and get
        a Spark session.  In this context, we are connecting to Spark in local mode.



- Ingesting

    - When you tell Spark to load the CSV file, the Master does not do the work.  Spark
        has the 'workers' (or 'slaves') do the work.  

    - In a cluster, a distributed ingestion will occur.  Each worker will read a portion of
        the CSV file and store it in its memory.



-Transforming

    - The transformations on the CSV rows will occur on the rows in memory where they are 
        stored on each of the workers.



- Saving to the Database

    - Each worker creates its owner connection to the database and saves its results.  We have
        to be mindful that if we have too many workers, there may be too many simultaneous
        connections to the database.



- In Summary

    - The whole dataset never hits our application (the driver).
    
    - The entire processing takes place in the workers.

    - The workers save the data in their partitions to the database.