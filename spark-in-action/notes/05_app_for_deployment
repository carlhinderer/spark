-----------------------------------------------------------------------
|  CHAPTER 5 - BUILDING AN APP FOR DEPLOYMENT                         |
-----------------------------------------------------------------------

- An Ingestionless Example

    - In this chapter, we'll learn about the 3 ways to interact with Spark:

        1. Local mode (which we've been using)
        2. Cluster mode
        3. Interactive mode


    - To keep the cluster and deployment concepts simple, we will build an app that 
        doesn't need to ingest any data.  We will use Spark to self-generate a large
        random dataset to estimate Pi.


    - We use the Monte Carlo method of estimating Pi.  We get random points on a quadrant
        of the unit circle.  

        0 <= x <= 1
        0 <= y <= 1

      If sqrt(x**2 + y**2) <= 1, the point is within the unit circle.
      If sqrt(x**2 + y**2) > 1, the point is out of the unit circle.

      As we add more and more points, the fraction of points within the circle will approach Pi/4.


    - Our Spark application will:

        1. Open a Spark session.

        2. Use Spark to create a dataset contianing one row per dart throw.

        3. Create a dataset using the result of each throw.

        4. Sum the count of throws within the unit circle.

        5. Compute the ratio of throws in both zones and multiply it by 4 to approximate Pi.



- First Version of Pi Approximation

    - We'll slice up 1 million throws into 10 separate slices.  Remember that nothing will
        actually be executed until we call an action.

        # Create the slices
        slices = 10
        number_of_throws = 100000 * slices


    - Next, we'll create a Python list of integers, and convert it to a DataFrame to 
        parallelize the processing.

        # Create the SparkSession
        spark = SparkSession.builder.appName("PySpark Pi").master("local[*]").getOrCreate()

        # Create a Python list with 1 million integers
        num_list = []
        for x in range(number_of_throws):
            num_list.append(x)

        # Convert to a DataFrame to dispatch the processing over as many nodes as possible
        incremental_rdd = spark.sparkContext.parallelize(num_list)


    - Next, we'll create a function to throw the darts, and then map that function over each
        of our trials.

        # Function for throwing darts
        from random import random

        def throw_darts(_):
            x = random() * 2 - 1
            y = random() * 2 - 1
            return 1 if x ** 2 + y ** 2 <= 1 else 0

        # Map the function
        darts_rdd = incremental_rdd.map(throw_darts)


    - To get our result, we'll use the reduce function.

        # Get the results of the dart throws
        from operator import add

        darts_in_circle = darts_rdd.reduce(add)


    - Finally, we compute our estimation of Pi and print it.

        pi_estimate = 4.0 * darts_in_circle / number_of_throws
        print("Pi is roughly {}".format(pi_estimate)



- Using Lambda Functions

    - We can also pass lambda functions into the 'map' and 'reduce' operations to make
        the syntax simpler if we want.

        # Get the results of the dart throws
        darts_in_circle = darts_rdd.reduce(lambda a,b: a+b)



- Spark Local Mode

    - Local mode enables all Spark components to run on a single machine.

            Application Node
        ------------------------------
        |    Application Code        |
        |    Application JARs        |
        |    Spark Session           |
        |                            |
        |    Executor   [Cache]      |
        |            [Task] [Task]   |
        ------------------------------


    - Allows for fast onboarding of new developers, and is extremely useful for development
        in general.


    - By default, local mode will run with one thread.

        SparkSession spark = SparkSession.builder()
            .appName("My application")
            .master("local")
            .getOrCreate();


    - You can also pass in the number of threads you want to use (2 in this case).

        SparkSession spark = SparkSession.builder()
            .appName("My application")
            .master("local[2]")
            .getOrCreate();



- Spark Cluster Mode

    - In cluster mode, Spark is a multinode system with a master and workers.

    - The master dispatches the workload to the worker, which then processes it.

    - Starting several workers on the same worker node is possible, but it doesn't seem
        like there are any benefits to this.  A worker will use all of the available or
        configured resources.  


    - Here is what the cluster environment looks like:


                                                      Worker Node
                                                    -----------------------------
                                                    | Application JARs          |
                                                    |           [Cache]         |
           Application Node                         | Executor  [Task] [Task]   |
         ----------------------   ---------------   -----------------------------
         |  Application Code  |   | Master Node |
         |  Application JARs  |   ---------------     Worker Node
         |  SparkSession      |                     -----------------------------
         ----------------------                     | Application JARs          |
                                                    |           [Cache]         |
                                                    | Executor  [Task] [Task]   |
                                                    -----------------------------

                                                      Worker Node
                                                    -----------------------------
                                                    | Application JARs          |
                                                    |           [Cache]         |
                                                    | Executor  [Task] [Task]   |
                                                    -----------------------------


    - Note that the Application Node, Master Node, and Worker Node could all be the same
        physical node.


    - The Application Node contains the application code, which is called the 'driver program'
        as it drives Spark.  The application node also contains the addtional JARs your
        application needs.  The application will open a session on Spark using the Spark
        libraries.


    - The Master Node contains the Spark libraries, which include the code to run as a master.


    - The workers have the application JARs and the Spark libraries to execute the code.
        They also have the binaries to connect to the master: the worker scripts.



- Spark vs Hadoop

    - Hadoop

        - Open source and governed by ASF like Spark
        - Popular implementation of MapReduce
        - Complex ecosystem
        - Has constraints on the type of algorithms (mostly MapReduce) and storage (mostly disk)


    - Spark uses some Hadoop libraries, which are contained in the Spark runtime you are
        deploying on each node.



- Uber JARs

    - In some cases, to ease deployment, you will want to create an uber JAR containing your
        application and its dependencies.  In this case, your uber JAR should never include
        Hadoop or Spark libraries, as they will already be deployed.


    - A JAR is a Java archive file.  When you compile all of your .java files into a .class,
        you combine them all into a JAR file.


    - An uber JAR (aka a super JAR or fat JAR) contains most, if not all, of the dependencies
        of your application.



- Running Applications on a Cluster

    - There are 2 ways to run an application on a cluster:

        1. You submit a job using the 'spark-submit' shell and a JAR of your application.

        2. You specify the master in your application, then run your code.


    - Whether you submit a job or set the cluster's master in your application, you need to:

        1. Build a JAR of your application.

        2. Make sure all the JARs that your application depends on are on each node or in the
             uber JAR.



- Interactive Mode in Scala and Python

    - You can also use Spark in full interactive mode, which allows you to manipulate
        big data in a shell.  


    - In addition to the shell, you can use notebooks such as Jupyter and Apache Zeppelin.


    - Here is what the interactive environment looks like:


                                                      Worker Node
                                                    -----------------------------
                                                    | Application JARs          |
                                                    |           [Cache]         |
              Shell Node                            | Executor  [Task] [Task]   |
         ----------------------   ---------------   -----------------------------
         |  Interactive Code  |   | Master Node |
         |  Application JARs  |   ---------------     Worker Node
         |  SparkSession      |                     -----------------------------
         ----------------------                     | Application JARs          |
                                                    |           [Cache]         |
                                                    | Executor  [Task] [Task]   |
                                                    -----------------------------

                                                      Worker Node
                                                    -----------------------------
                                                    | Application JARs          |
                                                    |           [Cache]         |
                                                    | Executor  [Task] [Task]   |
                                                    -----------------------------
