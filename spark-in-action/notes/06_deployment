-----------------------------------------------------------------------
|  CHAPTER 6 - DEPLOYING THE SIMPLE APP                               |
-----------------------------------------------------------------------

- CI/CD

    - Continuous Integration

        - Merge all developers' working copies into the mainline at frequent intervals
        - XP recommends doing this more than once per day
        - Goal is prevent integration problems
        - Used in combination with unit tests and TDD
        - Eventually, build servers were created to run tests after each commit

    - Continuous Delivery

        - Produce software in short cycles
        - Software can be reliably released at any time, but decision is still made by human
        - Build, test, and release with greater frequency
        - Reduces cost, time, and risk

    - Continuous Deployment

        - Any change that passes test is automatically deployed to production



- Spark Components and Their Interactions


                ------------------------- #4 --------------------------------
                |                                                           |
                v                                                           |
    Driver or Application                                              Worker Node
                                                        ---- #2 --->            [Cache]
        SparkSession               Master/              |              Executor [Task] [Task]
       (SparkContext)---- #1 --->  ClusterManager  -----|                   |
               ^                                        |                   | #3
               |                                        |                   v
               |                                        --- #2 --->    Worker Node
               |                                                                [Cache]
               |                                                       Executor [Task] [Task]
               |                                                            |
               -------------------------- #4 --------------------------------


            #1 = The application connects to the cluster manager.

            #2 = The connection between the workers and master.  The workers initiate the 
                   connection, but data is passed from the master to the workers.

            #3 = Internal link between the executors

            #4 = The executors return results back to the driver.  We must configure firewalls
                   so that each worker can communicate with the driver.



- Example - Links in Pi Estimation Program

    private void start(int slices) {
        intnumberOfThrows = 100000 * slices;
        ...    

        # Link 1 - The session resides on the cluster manager
        SparkSession spark = SparkSession
            .builder()
            .appName("JavaSparkPi on a cluster")
            .master("spark://un:7077")
            .config("spark.executor.memory", "4g")
            .getOrCreate();

        ...

        List<Integer> l = new ArrayList<>(numberOfThrows);
        for (inti = 0; i < numberOfThrows; i++) {
            l.add(i);
        }

        # Link 2 - The first DataFrame is created in the executor
        Dataset<Row> incrementalDf = spark
            .createDataset(l, Encoders.INT())
            .toDF();

        # This step is added to the DAG, which sits in the Cluster Manager
        Dataset<Integer> dartsDs = incrementalDf
            .map(new DartMapper(), Encoders.INT());

        # Link 4 - The result of the reduce operation is brought back to the application
        intdartsInCircle = dartsDs.reduce(new DartReducer());

        # Print results and stop the session
        System.out.println("Pi is roughly " + 4.0 * dartsInCircle / numberOfThrows);
        spark.stop();



- Architecture Details

    - Spark applications run as independent processes on a cluster.  The 'SparkSession'
        object in your application (the driver) coordinates the processes.  There is a
        unique SparkSession for your application, whether you are in local mode or have
        10,000 nodes.

      The SparkSession is created when you build it.

        SparkSession spark = SparkSession.builder()
            .appName("An app")
            .master("local[*]")
            .getOrCreate();


    - As part of your session, you will also get a 'SparkContext'.  You mostly don't need
        to interact with it, but if you do (to get infrastructure information or create
        a custom accumulator), this is how:

        SparkContext sc = spark.sparkContext();
        System.out.println("Running Spark v" + sc.version());


    - The cluster manager allocates resources across applications.  To run on a cluster,
        the SparkSession can connect to several types of cluster managers.  These include
        Spark standalone, YARN, Mesos, and Kubernetes.

      One of the cluster manager's roles is to find worker nodes with resources available.


    - Once connected, Spark acquires executors on nodes in the cluster, which are JVM
        processes that run computations and store data for your application.

      Next, the cluster manager sends your application code to the executors.  You do not
        need to deploy your application on each node.

      Finally, SparkSession sends tasks to the executors to run.



- Tips for Troubleshooting Spark Architecture

    - The executors must always be able to talk to the driver, even if results are 
        written somewhere else besides the application itself.  They should not be
        isolated by a firewall, on a different network, exposing multiple IP addresses,
        etc.

      Note that communication problems can arise when you are connecting from your 
        development machine to the cluster.


    - Each application gets its own executor processes, which stay up for the duration of
        the entire application.  This has the advantage of isolating applications from 
        each other, but be aware that data cannot be shared across multiple Spark
        applications unless it is written to an external storage system.


    - Spark is agnostic to the underlying cluster manager, as long as it can acquire
        executor processes and they can communicate with each other.


    - As the driver schedules tasks on the cluster, it should run physically close to the
        worker nodes, preferably on the same LAN.  Processes are network intensive, and
        you reduce latency when the boxes are close together.  This can be tricky in 
        cloud installations.



- Building a Cluster

    - Now, we'll learn how to:

        1. Build a cluster
        2. Set its environment
        3. Deploy your application
        4. Run your application
        5. Analyze the execution logs


    - Where should the cluster be installed?

        - 4 nodes total in this example
        - Can use cloud VMs (target 8GB RAM and 32GB disk)



- Setting Up the Environment

    - Now that we have defined our environment, we will need to:

        1. Install Spark
        2. Configure and run Spark
        3. Download/upload our application
        4. Run it


    - Workers are aka 'slaves'.  This terminology is being phased out, but you'll still see it
        in command-line scripts.


    - We'll assume we have these 4 nodes:

        Node #1
          hostname: un
          FQDN: un.oplo.io
          IP: 10.0.100.81
          Master and Worker1

        Node #2
          hostname: deux
          FQDN: deux.oplo.io
          IP: 10.0.100.82
          Worker2

        Node #3
          hostname: trois
          FQDN: trois.oplo.io
          IP: 10.0.100.83
          Worker3

        Node #4
          hostname: quatre
          FQDN: quatre.oplo.io
          IP: 10.0.100.84
          Worker4


    - We will need to install Spark on every node.  Installation is pretty straightforward.
        We do not need to install our application on every node.

      For the rest of this chapter, we'll assume Spark has been installed in
        /opt/apache-spark.


    - On the master node, run the master:

        $ cd /opt/apache-spark/sbin
        $ ./start-master.sh


    - Remember that the master does not do much.  So, in this scenario, we'll run a worker
        on the same node also.

        $ ./start-slave.sh spark://un.7077


    - We should check that the ports we need are accessible and not being used by something
        else.  Typically, they are 7077, 8080, and 4040.  These ports do not need to be
        open to the internet, just internally.  We can use ping and telnet to check them.


    - The master will always need to be started first.  To check that everything is OK, 
        we navigate in a browser to:

        http://un:8080/

      The web interface is provided by Spark, we don't need to configure a web server or 
        anything.


    - Now, we can go to the next node and start the second worker.

        $ cd /opt/apache-spark/sbin
        $ ./start-slave.sh spark://un:7077


    - Now, if we refresh our browser, we should see the results.  We can start the third and
        fourth workers, and our physical cluster will be up and running.