-----------------------------------------------------------------------
|  CHAPTER 6 - DEPLOYING THE SIMPLE APP                               |
-----------------------------------------------------------------------

- CI/CD

    - Continuous Integration

        - Merge all developers' working copies into the mainline at frequent intervals
        - XP recommends doing this more than once per day
        - Goal is prevent integration problems
        - Used in combination with unit tests and TDD
        - Eventually, build servers were created to run tests after each commit

    - Continuous Delivery

        - Produce software in short cycles
        - Software can be reliably released at any time, but decision is still made by human
        - Build, test, and release with greater frequency
        - Reduces cost, time, and risk

    - Continuous Deployment

        - Any change that passes test is automatically deployed to production



- Spark Components and Their Interactions


                ------------------------- #4 --------------------------------
                |                                                           |
                v                                                           |
    Driver or Application                                              Worker Node
                                                        ---- #2 --->            [Cache]
        SparkSession               Master/              |              Executor [Task] [Task]
       (SparkContext)---- #1 --->  ClusterManager  -----|                   |
               ^                                        |                   | #3
               |                                        |                   v
               |                                        --- #2 --->    Worker Node
               |                                                                [Cache]
               |                                                       Executor [Task] [Task]
               |                                                            |
               -------------------------- #4 --------------------------------


            #1 = The application connects to the cluster manager.

            #2 = The connection between the workers and master.  The workers initiate the 
                   connection, but data is passed from the master to the workers.

            #3 = Internal link between the executors

            #4 = The executors return results back to the driver.  We must configure firewalls
                   so that each worker can communicate with the driver.



- Example - Links in Pi Estimation Program

    private void start(int slices) {
        intnumberOfThrows = 100000 * slices;
        ...    

        # Link 1 - The session resides on the cluster manager
        SparkSession spark = SparkSession
            .builder()
            .appName("JavaSparkPi on a cluster")
            .master("spark://un:7077")
            .config("spark.executor.memory", "4g")
            .getOrCreate();

        ...

        List<Integer> l = new ArrayList<>(numberOfThrows);
        for (inti = 0; i < numberOfThrows; i++) {
            l.add(i);
        }

        # Link 2 - The first DataFrame is created in the executor
        Dataset<Row> incrementalDf = spark
            .createDataset(l, Encoders.INT())
            .toDF();

        # This step is added to the DAG, which sits in the Cluster Manager
        Dataset<Integer> dartsDs = incrementalDf
            .map(new DartMapper(), Encoders.INT());

        # Link 4 - The result of the reduce operation is brought back to the application
        intdartsInCircle = dartsDs.reduce(new DartReducer());

        # Print results and stop the session
        System.out.println("Pi is roughly " + 4.0 * dartsInCircle / numberOfThrows);
        spark.stop();



- Architecture Details

    - Spark applications run as independent processes on a cluster.  The 'SparkSession'
        object in your application (the driver) coordinates the processes.  There is a
        unique SparkSession for your application, whether you are in local mode or have
        10,000 nodes.

      The SparkSession is created when you build it.

        SparkSession spark = SparkSession.builder()
            .appName("An app")
            .master("local[*]")
            .getOrCreate();


    - As part of your session, you will also get a 'SparkContext'.  You mostly don't need
        to interact with it, but if you do (to get infrastructure information or create
        a custom accumulator), this is how:

        SparkContext sc = spark.sparkContext();
        System.out.println("Running Spark v" + sc.version());


    - The cluster manager allocates resources across applications.  To run on a cluster,
        the SparkSession can connect to several types of cluster managers.  These include
        Spark standalone, YARN, Mesos, and Kubernetes.

      One of the cluster manager's roles is to find worker nodes with resources available.


    - Once connected, Spark acquires executors on nodes in the cluster, which are JVM
        processes that run computations and store data for your application.

      Next, the cluster manager sends your application code to the executors.  You do not
        need to deploy your application on each node.

      Finally, SparkSession sends tasks to the executors to run.



- Tips for Troubleshooting Spark Architecture

    - The executors must always be able to talk to the driver, even if results are 
        written somewhere else besides the application itself.  They should not be
        isolated by a firewall, on a different network, exposing multiple IP addresses,
        etc.

      Note that communication problems can arise when you are connecting from your 
        development machine to the cluster.


    - Each application gets its own executor processes, which stay up for the duration of
        the entire application.  This has the advantage of isolating applications from 
        each other, but be aware that data cannot be shared across multiple Spark
        applications unless it is written to an external storage system.


    - Spark is agnostic to the underlying cluster manager, as long as it can acquire
        executor processes and they can communicate with each other.


    - As the driver schedules tasks on the cluster, it should run physically close to the
        worker nodes, preferably on the same LAN.  Processes are network intensive, and
        you reduce latency when the boxes are close together.  This can be tricky in 
        cloud installations.