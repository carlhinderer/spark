-----------------------------------------------------------------------
|  CHAPTER 8 - INGESTION FROM DATABASES                               |
-----------------------------------------------------------------------

- Adding the Database Drivers

    - In this chapter, we'll see examples of connecting to:

        - MySQL (a SQL database)
        - Informix (requires a custom dialect)
        - ElasticSearch (a NoSQL database)


    - Here is an example of a 'pom.xml' file that includes the drivers for connecting to these
        platforms:


        pom.xml
        ----------------------------------------------------------
        ...
        <properties>
            ...
            <mysql.version>8.0.8-dmr</mysql.version>
            <informix-jdbc.version>4.10.8.1</informix-jdbc.version>
            <elasticsearch-hadoop.version>6.2.1</elasticsearch-hadoop.version>
        </properties>
        <dependencies>
            ...
            <dependency>
                <groupId>mysql</groupId>
                <artifactId>mysql-connector-java</artifactId>
                <version>${mysql.version}</version>
            </dependency>
            <dependency>
                <groupId>com.ibm.informix</groupId>
                <artifactId>jdbc</artifactId>
                <version>${informix-jdbc.version}</version>
            </dependency>
            <dependency>
                <groupId>org.elasticsearch</groupId>
                <artifactId>elasticsearch-hadoop</artifactId>
                <version>${elasticsearch-hadoop.version}</version> 
            </dependency>



- Ingestion from Relational Databases

    - Database Connections

        - Spark connects directly to relational databases using JDBC drivers.  They 
            require the following:

            - The JDBC driver in the classpath of the workers
            - The connection URL
            - The username
            - The password


    - Understanding the Example Data

        - We'll use the 'Sakila' sample database that comes with MySQL.  It represents a
            DVD rental store.  It has about 15 tables and some views.


        - We'll use the 'actor' table for this project.

            actor
            ---------------------------------------------
            actor_id       SMALLINT        (Primary Key)
            first_name     VARCHAR(45)
            last_name      VARCHAR(45)
            last_update    TIMESTAMP


    - Connection Method #1 - Use variables for database options

        # Database connection settings
        user = "root"
        password = "Spark<3Java"
        use_ssl="false"
        mysql_url = "jdbc:mysql://localhost:3306/sakila?serverTimezone=EST"
        dbtable = "actor"
        database="sakila"
        
        # Ingest the actor table from the database
        df = spark.read.format("jdbc") \
                .options(url=mysql_url,
                         database=database,
                         dbtable=dbtable,
                         user=user,
                         password=password) \
                .load()


    - Connection Method #2 - Using the Long MySQL URL

        # Database settings
        mysql_long_url = """
                jdbc:mysql://localhost:3306/sakila?user=root
                &password=Spark<3Java
                &useSSL=false
                &serverTimezone=EST
            """
        
        dbtable = "actor"
        
        # Ingest the actor table from the database
        df = spark.read.format("jdbc") \
            .options(url=mysql_long_url,
                     dbtable=dbtable)  \
            .load()


    - Connection Method #3 - Options Hard-coded in the DataFrame Read

        df = spark.read.format("jdbc") \
                  .option("url", "jdbc:mysql://localhost:3306/sakila") \
                  .option("dbtable", "actor") \
                  .option("user", "root") \
                  .option("password", "Spark<3Java") \
                  .option("useSSL", "false") \
                  .option("serverTimezone", "EST") \
                  .load()



- The Role of the Dialect

    - A 'dialect' is a translation block between Spark and the database.  It's typically a
        small component, implemented in a single class.  The dialect defines Spark's behavior
        when communicating with the database. 


    - For example, Postgres and Informix have a 'SERIAL' type, which holds an auto-incrementing
        integer used for primary keys.  Spark needs to be told that this type is an integer
        when it hits Tungsten storage.

      Tungsten Storage is the storage manager that optimizes memory usage for objects, bypassing
        the JVM's storage mechanism for more efficiency.


    - Spark comes with a few database dialects as part of the standard distribution.  As of
        Spark v3.0.0, they include:

        - IBM DB2
        - Apache Derby
        - MySQL
        - Microsoft SQL Server
        - Oracle
        - PostgreSQL
        - Teradata Database



- Building your own Dialect

    - We will create an Informix dialect, since there is no officially supported one.
        It is one class, 'InformixJdbcDialect' , that extends the 'JdbcDialect' class.


    - Note that Spark is written in Scala, so we'll need to return Scala types.


    - Catalyst is the optimization engine built in Spark.  The 'getCatalystType()' method
        receives 4 arguments:

        1. The SQL type as an integer, as defined in 'java.sql.Types'
        2. The type name as a string
        3. The size for numeric, string, and binary types
        4. A metadata builder


    - The 'getCatalystType()' method return type is one of the following:

        1. A datatype:

            BinaryType
            BooleanType
            Byte-Type
            CalendarIntervalType
            DateType
            DoubleType
            FloatType
            IntegerType
            LongType
            NullType
            ShortType
            StringType
            TimestampType

        2. A subtype of 'org.apache.spark.sql.types.DataType':

            ArrayType
            HiveStringType
            MapType
            NullType
            NumericTypeObjectType
            StructType



- Using the Custom Dialect

    - Now, we can use the custom dialect we created:

        // Specific Informix dialect
        JdbcDialect dialect = new InformixJdbcDialect();
        JdbcDialects.registerDialect(dialect);

        // Using properties
        Dataset<Row> df = spark
            .read()
            .format("jdbc")
            .option(
                "url",
                "jdbc:informix-sqli://[::1]:33378/stores_demo:IFXHOST=lo_informix1210;DELIMIDENT=Y")
            .option("dbtable", "customer")
            .option("user", "informix")
            .option("password", "in4mix")
            .load();