-----------------------------------------------------------------------
|  CHAPTER 8 - INGESTION FROM DATABASES                               |
-----------------------------------------------------------------------

- Adding the Database Drivers

    - In this chapter, we'll see examples of connecting to:

        - MySQL (a SQL database)
        - Informix (requires a custom dialect)
        - ElasticSearch (a NoSQL database)


    - Here is an example of a 'pom.xml' file that includes the drivers for connecting to these
        platforms:


        pom.xml
        ----------------------------------------------------------
        ...
        <properties>
            ...
            <mysql.version>8.0.8-dmr</mysql.version>
            <informix-jdbc.version>4.10.8.1</informix-jdbc.version>
            <elasticsearch-hadoop.version>6.2.1</elasticsearch-hadoop.version>
        </properties>
        <dependencies>
            ...
            <dependency>
                <groupId>mysql</groupId>
                <artifactId>mysql-connector-java</artifactId>
                <version>${mysql.version}</version>
            </dependency>
            <dependency>
                <groupId>com.ibm.informix</groupId>
                <artifactId>jdbc</artifactId>
                <version>${informix-jdbc.version}</version>
            </dependency>
            <dependency>
                <groupId>org.elasticsearch</groupId>
                <artifactId>elasticsearch-hadoop</artifactId>
                <version>${elasticsearch-hadoop.version}</version> 
            </dependency>



- Ingestion from Relational Databases

    - Database Connections

        - Spark connects directly to relational databases using JDBC drivers.  They 
            require the following:

            - The JDBC driver in the classpath of the workers
            - The connection URL
            - The username
            - The password


    - Understanding the Example Data

        - We'll use the 'Sakila' sample database that comes with MySQL.  It represents a
            DVD rental store.  It has about 15 tables and some views.


        - We'll use the 'actor' table for this project.

            actor
            ---------------------------------------------
            actor_id       SMALLINT        (Primary Key)
            first_name     VARCHAR(45)
            last_name      VARCHAR(45)
            last_update    TIMESTAMP


    - Connection Method #1 - Use variables for database options

        # Database connection settings
        user = "root"
        password = "Spark<3Java"
        use_ssl="false"
        mysql_url = "jdbc:mysql://localhost:3306/sakila?serverTimezone=EST"
        dbtable = "actor"
        database="sakila"
        
        # Ingest the actor table from the database
        df = spark.read.format("jdbc") \
                .options(url=mysql_url,
                         database=database,
                         dbtable=dbtable,
                         user=user,
                         password=password) \
                .load()


    - Connection Method #2 - Using the Long MySQL URL

        # Database settings
        mysql_long_url = """
                jdbc:mysql://localhost:3306/sakila?user=root
                &password=Spark<3Java
                &useSSL=false
                &serverTimezone=EST
            """
        
        dbtable = "actor"
        
        # Ingest the actor table from the database
        df = spark.read.format("jdbc") \
            .options(url=mysql_long_url,
                     dbtable=dbtable)  \
            .load()


    - Connection Method #3 - Options Hard-coded in the DataFrame Read

        df = spark.read.format("jdbc") \
                  .option("url", "jdbc:mysql://localhost:3306/sakila") \
                  .option("dbtable", "actor") \
                  .option("user", "root") \
                  .option("password", "Spark<3Java") \
                  .option("useSSL", "false") \
                  .option("serverTimezone", "EST") \
                  .load()



- The Role of the Dialect

    - A 'dialect' is a translation block between Spark and the database.  It's typically a
        small component, implemented in a single class.  The dialect defines Spark's behavior
        when communicating with the database. 


    - For example, Postgres and Informix have a 'SERIAL' type, which holds an auto-incrementing
        integer used for primary keys.  Spark needs to be told that this type is an integer
        when it hits Tungsten storage.

      Tungsten Storage is the storage manager that optimizes memory usage for objects, bypassing
        the JVM's storage mechanism for more efficiency.


    - Spark comes with a few database dialects as part of the standard distribution.  As of
        Spark v3.0.0, they include:

        - IBM DB2
        - Apache Derby
        - MySQL
        - Microsoft SQL Server
        - Oracle
        - PostgreSQL
        - Teradata Database



- Building your own Dialect

    - We will create an Informix dialect, since there is no officially supported one.
        It is one class, 'InformixJdbcDialect' , that extends the 'JdbcDialect' class.


    - Note that Spark is written in Scala, so we'll need to return Scala types.


    - Catalyst is the optimization engine built in Spark.  The 'getCatalystType()' method
        receives 4 arguments:

        1. The SQL type as an integer, as defined in 'java.sql.Types'
        2. The type name as a string
        3. The size for numeric, string, and binary types
        4. A metadata builder


    - The 'getCatalystType()' method return type is one of the following:

        1. A datatype:

            BinaryType
            BooleanType
            Byte-Type
            CalendarIntervalType
            DateType
            DoubleType
            FloatType
            IntegerType
            LongType
            NullType
            ShortType
            StringType
            TimestampType

        2. A subtype of 'org.apache.spark.sql.types.DataType':

            ArrayType
            HiveStringType
            MapType
            NullType
            NumericTypeObjectType
            StructType



- Using the Custom Dialect

    - Now, we can use the custom dialect we created:

        // Specific Informix dialect
        JdbcDialect dialect = new InformixJdbcDialect();
        JdbcDialects.registerDialect(dialect);

        // Using properties
        Dataset<Row> df = spark
            .read()
            .format("jdbc")
            .option(
                "url",
                "jdbc:informix-sqli://[::1]:33378/stores_demo:IFXHOST=lo_informix1210;DELIMIDENT=Y")
            .option("dbtable", "customer")
            .option("user", "informix")
            .option("password", "in4mix")
            .load();



- Filtering a MySQL Table using a WHERE Clause

    - Here, we use a WHERE clause to restrict the data we are pulling back from the database,
        since ingestion is an expensive process.

        # Query with WHERE clause
        sqlQuery = """
            select * from film where 
            (title like \"%ALIEN%\" or title like \"%victory%\" 
            or title like \"%agent%\" or description like \"%action%\") 
            and rental_rate>1 
            and (rating=\"G\" or rating=\"PG\")
        """
        
        # Ingest from the actor table
        df = spark.read.format("jdbc") \
            .option("url", mysql_url) \
            .option("user", user) \
            .option("dbtable", dbtable) \
            .option("password", password) \
            .load()
        
        df = df.select(sqlQuery)



- Joining Data in the Database

    - Spark can join 2 DataFrames together, but for performance and optimization reasons, 
        you may want to do the joins in the database.  


    - This is the query we want to run:

        SELECT actor.first_name, actor.last_name, film.title, film.description 
        FROM actor, film_actor, film 
        WHERE actor.actor_id = film_actor.actor_id
        AND film_actor.film_id = film.film_id


    - Here is how we perform the join:

        # Read MySQL tables
        df = spark.read.format("jdbc") \
            .option("url", mysql_url) \
            .option("user", user) \
            .option("dbtable", dbtable) \
            .option("password", password) \
            .load()
        
        # Builds the SQL query doing the join operation
        sqlQuery = """
            select actor.first_name, actor.last_name, film.title, 
            film.description 
            from actor, film_actor, film 
            where actor.actor_id = film_actor.actor_id 
            and film_actor.film_id = film.film_id
        """
        
        df = df.select(sqlQuery)



- Performing Ingesting and Partitioning

    - We can also ingest from a database and automatically assign to the partitions.  By default,
        we will load all the data from the table into a single partition.  However, we can 
        immediately split the data into partitions if we need to.


        # Partition settings
        partition_column = "film_id"
        lower_bound = 1
        upper_bound = 1000
        num_partitions = 10
        
        # Read from the film table
        df = spark.read.format("jdbc") \
            .option("url", mysql_url) \
            .option("user", user) \
            .option("dbtable", dbtable) \
            .option("password", password) \
            .option("partitionColumn", partition_column) \
            .option("lowerBound", lower_bound) \
            .option("upperBound", upper_bound) \
            .option("numPartitions", num_partitions) \
            .load()



- Ingestion from Elasticsearch

    - Elasticsearch is a scalable document store and search engine.  A bidirectional
        communication Elasticsearch helps Spark store and retrieve complex documents.


    - We'll need to add the Elasticsearch driver to the 'pom.xml' file.

        <dependency>
            <groupId>org.elasticsearch</groupId>
            <artifactId>elasticsearch-hadoop</artifactId>
            <version>6.2.1</version>
        </dependency>


    - Elasticsearch stores documents in JSON format.


    - Here, we ingest from Elasticsearch:

        df = spark.read.format("org.elasticsearch.spark.sql") \
                  .option("es.nodes", "localhost") \
                  .option("es.port", "9200") \
                  .option("es.query", "?q=*") \
                  .option("es.read.field.as.array.include", "Inspection_Date") \
                  .load("nyc_restaurants")