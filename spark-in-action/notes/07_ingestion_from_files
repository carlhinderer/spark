-----------------------------------------------------------------------
|  CHAPTER 7 - INGESTION FROM FILES                                   |
-----------------------------------------------------------------------

- Common Behavior of Parsers

    - We can use regexes in the file paths.  For instance, if we use 'a*' in the path,
        all files starting with 'a' will be ingested.

    - Options are not case-sensitive.  So 'multiline' and 'multiLine' are the same thing.



- Complex Ingestion From CSV

    - CSV has been around forever, and it has many variations:

        - the separators are not always commas
        - some records span multiple lines
        - there are various ways of escaping the separator


    - Here is the csv load we are using for this exercise:

        df = spark.read.format("csv") \
                  .option("header", "true") \
                  .option("multiline", True) \             # Some records split over multiple lines
                  .option("sep", ";") \                    # Separator is ;
                  .option("quote", "*") \                  # Escape character is *
                  .option("dateFormat", "MM/dd/yyyy") \    # Data matches MM/dd/yyyy format
                  .option("inferSchema", True) \
                  .load(CSV_FILE_PATH)


    - Note that even though Spark is inferring the schema, we still need to give it a hint
        about the date format so that the dates get parsed correctly.



- Ingesting a CSV with a Known Schema

    - We can infer the schema, but if we know the schema ahead of time, it can be useful
        to specify the datatypes by telling Spark what schema to use.


    - Here we create the schema:

        schema = StructType([StructField('id', IntegerType(), False),
                     StructField('authorId', IntegerType(), True),
                     StructField('bookTitle', IntegerType(), False),
                     StructField('releaseDate', DateType(), True),
                     StructField('url', StringType(), False)])


    - A StructField object comprises three fields, name (a string), dataType (a DataType) 
        and nullable (a bool).


    - Then we can use the schema when loading the csv:

        df = spark.read.format("csv") \
                  .option("header", True) \
                  .option("multiline", True) \
                  .option("sep", ";") \
                  .option("dateFormat", "MM/dd/yyyy") \
                  .option("quote", "*") \
                  .schema(schema) \
                  .load(absolute_file_path)



- Ingesting a JSON File

    - With the popularity of REST, JSON has become an extremely popular data exchange 
        format.  It is easier to read, less verbose, and has fewer constraints than XML.


    - There is a subformat of JSON called 'JSON Lines'.  JSON Lines stores a record on
        one line, which makes for easy parsing and readability.


    - Loading this format is very simple:

        df = spark.read.format("json") \
                  .load(JSON_FILE_PATH)


    - We can also access nested structures in the JSON document directly:

        new_df = df.withColumn("year", col("fields.year")) \
                   .withColumn("coordinates", col("geometry.coordinates"))



- Ingesting a Multiline JSON File

    - As of v2.2, Spark can also ingest multiline JSON files.

        df = spark.read.format("json") \
                  .option("multiline", True) \
                  .load(JSON_FILE_PATH)


    - Note that if you accidentally forget the 'multiline' option, you'll get a DataFrame
        with a single column '_corrupt_record'.

        +--------------------+
        |     _corrupt_record|
        +--------------------+
        |                 [ {|
        |       "tag" : "A1",|
        |  "geopoliticalar...|
        +--------------------+



- Ingesting an XML File

    - Years ago, it seemed like XML might become the lingua france of data exchange.  It
        turned out to be more verbose and harder to read than JSON, but it is still widely
        used.  XML is:

        - structured
        - extensible
        - self-describing
        - embeds schema rules via DTD and XSD
        - a W3 standard


    - In the XML we are using for this example, we have a sequence of rows enclosed in a
        root element:

        <response>
            <row
              _id="1"
              _uuid="BAC69188-84A6-4D28-951E-FC687ACB6D4A"
              _position="1"
              _address="https://data.nasa.gov/resource/nasa-patents/1">
                 <center>NASA Ames Research Center</center>
                 <status>Issued</status>
                 <case_number>ARC-14048-1</case_number>
                 <patent_number>5694939</patent_number>
                 <application_sn>08/543,093</application_sn>
                 <title>Autogenic-Feedback Training Exercise Method &amp; System</title>
                 <patent_expiration_date>2015-10-03T00:00:00</patent_expiration_date>
            </row>
            ...
        </response>


    - Since Spark doesn't read XML out of the box, we'll need to use the DataBricks XML package
        to read it.  Here is the loading of the DataFrame:

        df = spark.read.format("xml") \
                  .option("rowTag", "row") \
                  .load(XML_FILE_PATH)


    - And here we run it, including the DataBricks package:

        spark-submit --master local[*] \
                     --packages com.databricks:spark-xml_2.11:0.6.0 \
                     xml_ingestion.py



- Ingesting a Text File

    - With the rise of AI technologies like NLP, processing raw text files is becoming more
        popular.  


    - In this example, we read 'Romeo and Juliet' from Project Guttenberg.  Each line of the
        book will become a record in our DataFrame.  There is no feature to cut by sentence
        or word.

        df = spark.read.format("text") \
                  .load(TEXT_FILE_PATH)



- The Problem with Traditional File Formats

    - JSON and XML are not easy to split.  When you want your nodes to read part of a file,
        it is easier if you can split it.  Because of the root element, XML will need rework,
        which may break the document.  Big data files need to be splittable.


    - CSV cannot store hierarchical information like JSON and CSV can.


    - None of these file types are designed to incorporate metadata.


    - None of the file formats support easy column addition, deletion, or insertion.


    - These formats are quite verbose (especially XML and JSON), which inflates the file size
        drastically.


    - You can't use the binary formats used by most RDBMS's, since every vendor has its own
        format.



- Apache Avro is a Schema-Based Serialization Format

    - Avro is a data serialization system, which provides rich data structures in a compact,
        fast, binary data format.


    - Avro was designed for RPCs in a similar way as Google's Protocol Buffers, a popular method
        for transferring serializable data.


    - Avro offers a schema written in JSON.  Avro also supports dynamic modification of the schema.
        Because it is row-based, it is easy to split.



- ORC is a Columnar Storage Format

    - Apache ORC (Optimized Row Columnar) is a columnar storage format.  It is ACID-compliant.


    - Beyond the stadard datatypes, ORC supports compound types like structs, lists, maps, and
        unions.  ORC supports compression, which reduces file size and network transfer time.


    - ORC was backed by Hortonworks, which was bought by Cloudera.  So, the future of ORC is
        not yet known.



- Parquet is also a Columnar Storage Format

    - Like ORC, Apache Parquet is a columnar file format.  It also supports compression, and
        you can add columns at the end of the schema.  Parquet also supports compound types
        like lists and maps.


    - Parquet is backed by Cloudera, so the future is also unknown, following the merger.



- Comparing Avro, ORC, and Parquet

    - ORC, Parquet, and Avro share:

        - They are all binary formats.
        - They embed their schema.  Avro's JSON-based schema offers the most flexibility.
        - The compress their data.  ORC and Parquet are better at this than Avro.


    - Parquet is probably the most popular of the 3, but that may change over time.



- Ingesting Avro

    - To read the Avro file:

        df = spark.read.format("avro") \
                  .load(AVRO_FILE_PATH)


    - The Avro format is not directly supported by Spark, so we have to add the external
        package:

        $ spark-submit --master local[*] \
                       --packages org.apache.spark:spark-avro_2.11:2.4.0 \
                       avro_ingestion.py



- Ingesting ORC

- Ingesting Parquet