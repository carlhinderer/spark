-----------------------------------------------------------------------
|  CHAPTER 3 - DATAFRAMES                                             |
-----------------------------------------------------------------------

- The Essential Role of the DataFrame

    - DataFrames are both a data structure and an API.  It is used in a unified way
        by Spark SQL, Spark Streaming, MLLib, and GraphX.  The DataFrame API is
        available in Java, Python, Scala, and R.

    - DataFrames are a set of records organized into named columns, similarly to a
        relational database.

    - Can be constructed from a wide range of sources, such as files, databases, or
        custom sources.

    - Can be stored in memory or on disk, but will use as much memory as it can.



- Immutability

    - DataFrames, as well as datasets and RDDs, are immutable.

    - In the first state, data is immuatable.  Then you start modifying it, but Spark
        only stores your modifications, not every step of the transformed data.

    - Immutability becomes really important when you think about it in a distributed
        way.  In terms of storage, you have 2 choices:

        1. You store data, and each modification is done immediately on every node, as
             in a relational DB.

        2. You keep the data in sync over the nodes and share only the transformation
             recipe with the various nodes.  This is what Spark does.



- DataFrame Example

    - In this example, we use 2 lists of restaurants, one from Wake County and one from 
        Durham County.  We need to transform them to have the same schema, then union
        them together.

        Data Files -> INGESTION -> TRANSFORMATION -> UNION -> DISPLAY


    - Once we have loaded the csv into the DataFrame, we can inspect the schema.

        df.printSchema()

        root 
          |-- OBJECTID: string (nullable = true) 
          |-- HSISID: string (nullable = true) 
          |-- NAME: string (nullable = true) 
          |-- ADDRESS1: string (nullable = true) 
          |-- ADDRESS2: string (nullable = true) 
          |-- CITY: string (nullable = true) 
          |-- STATE: string (nullable = true) 
          |-- POSTALCODE: string (nullable = true) 
          |-- PHONENUMBER: string (nullable = true) 
          |-- RESTAURANTOPENDATE: string (nullable = true) 
          |-- FACILITYTYPE: string (nullable = true) 
          |-- PERMITID: string (nullable = true) 
          |-- X: string (nullable = true) 
          |-- Y: string (nullable = true) 
          |-- GEOCODESTATUS: string (nullable = true)


    - We want to transform our schema so that it looks like:

        root 
          |-- datasetId: string (nullable = true)
          |-- name: string (nullable = true)
          |-- address1: string (nullable = true)
          |-- address2: string (nullable = true)
          |-- city: string (nullable = true)
          |-- state: string (nullable = true)
          |-- zip: string (nullable = true)
          |-- tel: string (nullable = true)
          |-- dateStart: string (nullable = true)
          |-- type: string (nullable = true)
          |-- geoX: string (nullable = true)
          |-- geoY: string (nullable = true)
          |-- county: string (nullable = false)
          |-- id: string (nullable = true)


    - We use these DataFrame methods in this example:

        withColumn()             # Creates new column from expression or column
        withColumnRenamed()      # Renames a column

        col()                    # Gets a column from its name
        drop()                   # Drops a column from a DataFrame

        lit()                    # Creates a literal value
        concat()                 # Concatenates the values in a set of columns



- Data is Stored in Partitions

    - Data is not physically stored in the DataFrame, it is stored in partitions.
        Partitions are not directly accessible through the DataFrames.  You have
        to look at the RDDs for that.

    - In our case, we only have a single partition when working locally.  In a cluster,
        the number of partitions is determined by the number of nodes and the size of
        the dataset.

    - To work with partitions directly:

        # Current number of partitions
        partition_count = df.rdd.getNumPartitions()

        # Repartition
        df = df.repartition(4)



- Digging in the Schema

    - We can get information about the schema in various formats.

        # Inspect the schema
        schema = df.schema
        
        # Schema as a tree
        df.printSchema()
        
        # Schema as a string
        print(df.schema)
        
        # Schema as json
        schema_as_json = df.schema.json()
        parsed_schema_as_json = json.loads(schema_as_json)
        print("*** Schema as JSON: {}".format(json.dumps(parsed_schema_as_json, indent=2)))



- A DataFrame After a JSON Ingestion

    - Ingesting JSON documents is a little more complex than CSVs because of their
        nested structure.  

    - To access fields in a structure, you can use dot notation.  To access an element 
        in an array, the 'getItem' function can be used.

        # Get a field
        .withColumn('datasetId', col('fields.id'))

        # Get an array element by position
        .withColumn('geoX', col('fields.geolocation').getItem(0))



- Combining Two DataFrames

    - Now, we want to combine the 2 DataFrames we have been using.  In order to do this,
        the schemas in the DataFrames must match exactly.  

    - The 'union()' method doesn't check the column names, it just unions the columns in 
        order.  This can lead to inconsistent data or a program stop if you're not 
        careful.

        >>> df = df1.union(df2)

    - The 'unionByName' is safer, since it actually matches up the columns by name.

        >>> df = df1.unionByName(df2)

    - Typically, when you load a small (<128 MB) dataset into a DataFrame, only one 
        partition will be created.  However, in this scenario, a partition will be created
        for each of the 2 original DataFrames.  Joining them will create a new unique
        DataFrame, which will rely on the 2 original partitions.



- The DataFrame is a Dataset<Row>

    - Datasets from any POJO (Plain Old Java Object) can be converted into a DataFrame.
        The DataFrame itself is a Dataset<Row>.

    - The advantage of using the 'Row' type is that is uses Tungsten for storage.  
        
    - Project Tungsten is an integrated part of Spark that focuses on memory management,
        binary processing, cache-aware computation, and code generation.  It manages
        large blocks of memory, as opposed to the JVM, which garbage collects individual
        objects.



- Creating a Dataset of Strings

    - To create a DataSet of strings:

        import java.util.Arrays;
        import java.util.List;

        import org.apache.spark.sql.Dataset;
        import org.apache.spark.sql.Encoders;

        # Create POJO
        String[] stringList = new String[] { "Jean", "Liz", "Pierre", "Lauric" };
        List<String> data = Arrays.asList(stringList);

        # Create DataSet from POJO
        Dataset<String> ds = spark.createDataset(data, Encoders.STRING());
        ds.show();
        ds.printSchema();

        # Convert DataSet to DataFrame
        Dataset<Row> df = ds.toDF();
        df.show();
        df.printSchema();



- Converting Back and Forth



- DataFrame's Ancestor - The RDD

    - Prior to DataFrames, Spark used RDDs exclusively.  RDDs were defined by Matei
        Zaharia as a 'distributed memory abstraction that lets programmers perform in-memory
        computations on large clusters in a fault-tolerant manner'.

    - The idea of RDDs was to build a reliable set of nodes.  If one fails, another takes the
        relay.  This is similar to the idea of a RAID 5 disk architecture. 

    - Dataframes and DataSets are built on top of RDDs. 

    - RDDs are much more difficult to work with than DataFrames, and should rarely be
        used directly.  You may want to use them when:

        1. You do not need a schema at all.
        2. You are developing low-level transformations and actions.
        3. You have legacy code.