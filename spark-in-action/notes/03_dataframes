-----------------------------------------------------------------------
|  CHAPTER 3 - DATAFRAMES                                             |
-----------------------------------------------------------------------

- The Essential Role of the DataFrame

    - DataFrames are both a data structure and an API.  It is used in a unified way
        by Spark SQL, Spark Streaming, MLLib, and GraphX.  The DataFrame API is
        available in Java, Python, Scala, and R.

    - DataFrames are a set of records organized into named columns, similarly to a
        relational database.

    - Can be constructed from a wide range of sources, such as files, databases, or
        custom sources.

    - Can be stored in memory or on disk, but will use as much memory as it can.



- Immutability

    - DataFrames, as well as datasets and RDDs, are immutable.

    - In the first state, data is immuatable.  Then you start modifying it, but Spark
        only stores your modifications, not every step of the transformed data.

    - Immutability becomes really important when you think about it in a distributed
        way.  In terms of storage, you have 2 choices:

        1. You store data, and each modification is done immediately on every node, as
             in a relational DB.

        2. You keep the data in sync over the nodes and share only the transformation
             recipe with the various nodes.  This is what Spark does.



- DataFrame Example

    - In this example, we use 2 lists of restaurants, one from Wake County and one from 
        Durham County.  We need to transform them to have the same schema, then union
        them together.

        Data Files -> INGESTION -> TRANSFORMATION -> UNION -> DISPLAY


    - Once we have loaded the csv into the DataFrame, we can inspect the schema.

        df.printSchema()

        root 
          |-- OBJECTID: string (nullable = true) 
          |-- HSISID: string (nullable = true) 
          |-- NAME: string (nullable = true) 
          |-- ADDRESS1: string (nullable = true) 
          |-- ADDRESS2: string (nullable = true) 
          |-- CITY: string (nullable = true) 
          |-- STATE: string (nullable = true) 
          |-- POSTALCODE: string (nullable = true) 
          |-- PHONENUMBER: string (nullable = true) 
          |-- RESTAURANTOPENDATE: string (nullable = true) 
          |-- FACILITYTYPE: string (nullable = true) 
          |-- PERMITID: string (nullable = true) 
          |-- X: string (nullable = true) 
          |-- Y: string (nullable = true) 
          |-- GEOCODESTATUS: string (nullable = true)


    - We want to transform our schema so that it looks like:

        root 
          |-- datasetId: string (nullable = true)
          |-- name: string (nullable = true)
          |-- address1: string (nullable = true)
          |-- address2: string (nullable = true)
          |-- city: string (nullable = true)
          |-- state: string (nullable = true)
          |-- zip: string (nullable = true)
          |-- tel: string (nullable = true)
          |-- dateStart: string (nullable = true)
          |-- type: string (nullable = true)
          |-- geoX: string (nullable = true)
          |-- geoY: string (nullable = true)
          |-- county: string (nullable = false)
          |-- id: string (nullable = true)


    - We use these DataFrame methods in this example:

        withColumn()             # Creates new column from expression or column
        withColumnRenamed()      # Renames a column

        col()                    # Gets a column from its name
        drop()                   # Drops a column from a DataFrame

        lit()                    # Creates a literal value
        concat()                 # Concatenates the values in a set of columns



- Data is Stored in Partitions

    - Data is not physically stored in the DataFrame, it is stored in partitions.
        Partitions are not directly accessible through the DataFrames.  You have
        to look at the RDDs for that.

    - In our case, we only have a single partition when working locally.  In a cluster,
        the number of partitions is determined by the number of nodes and the size of
        the dataset.

    - To work with partitions directly:

        # Current number of partitions
        partition_count = df.rdd.getNumPartitions()

        # Repartition
        df = df.repartition(4)



- Digging in the Schema

    - We can get information about the schema in various formats.

        # Inspect the schema
        schema = df.schema
        
        # Schema as a tree
        df.printSchema()
        
        # Schema as a string
        print(df.schema)
        
        # Schema as json
        schema_as_json = df.schema.json()
        parsed_schema_as_json = json.loads(schema_as_json)
        print("*** Schema as JSON: {}".format(json.dumps(parsed_schema_as_json, indent=2)))



- A DataFrame After a JSON Ingestion

    - Ingesting JSON documents is a little more complex than CSVs because of their
        nested structure.  

    - To access fields in a structure, you can use dot notation.  To access an element 
        in an array, the 'getItem' function can be used.

        # Get a field
        .withColumn('datasetId', col('fields.id'))

        # Get an array element by position
        .withColumn('geoX', col('fields.geolocation').getItem(0))



- Combining Two DataFrames


- The DataFrame is a Dataset<Row>


- Reusing POJOs


- Creating a Dataset of Strings


- Converting Back and Forth


- DataFrame's Ancestor - The RDD